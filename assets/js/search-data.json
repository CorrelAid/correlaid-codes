{
  
    
        "post0": {
            "title": "Breakthrough infections and Bayes' Theorem",
            "content": "Vaccine efficiencies and breakthrough infections . As part of the vaccination efforts during the Covid pandemic, vaccine efficiency and breakthrough infections have gotten a lot of attention. In this post we&#39;ll outline how Bayes&#39; theorem can help to understand these concepts and their relationship. For that purpose we&#39;ll first cast the concepts into a probabilistic framework with the following definitions . $I$ event of somebody being infected | $V$ event of somebody being vaccinated | $e$ efficiency of vaccination | $P(I| text{not } V)$ probability of an non-vaccinated person getting infected | $P(I|V)$ probability of a vaccinated person getting infected | $P(I|V) = (1-e)P(I| text{not }V)$ relationship between vaccination efficiency and probabilities | $P(V|I)$ probability of a infected person being vaccinated, i.e. chance of a breakthrough infection | . With these definitions in place all we need is Bayes&#39; Theorem . Bayes&#39; Theorem . $$ P(A|B) = frac {P(B|A)P(A)} {P(B|A)P(A) + P(B| text{not }A)P( text{not }A)} $$ . We can now apply this to breakthrough infections. For this purpose we set $A = V$ and $B = I$. That way we get . $$ P(V|I) = frac {P(I|V)P(V)} {P(I|V)P(V) + P(I| text{not }V)P( text{not }V)} .$$ . Plugging in the above definitions we can write this as . $$ begin{aligned} P(V|I) &amp;= frac {(1-e)P(I| text{not }V)P(V)} {(1-e)P(I| text{not }V)P(V) + P(I| text{not }V)P( text{not }V)} &amp;= frac {(1-e)P(V)} {(1-e)P(V) + P( text{not }V)} &amp;= frac {(1-e)q} {(1-e)q + (1-q)} end{aligned}.$$Here we set $P(V) = q$ which is the probability of a person being vaccinated that can be set to the percentage of people in a population begin vaccinated. . The above formula can be used to estimate the average efficiency of vaccinations given the percentage of people that where vaccinated and the percentage of breakthrough infections. To this end we replace the probability of a breakthrough infection with the percentage of observed breakthrough infections and for brevity call that variable $b = P(V|I)$. Plugging this into the above equation and solving for $e$ yields. . $$e = 1 - frac {(1-q)b} {(1-b)q} $$ . def avg_vaccine_efficiency(pct_vaccinated, pct_breakthroughs): return 100 * ( 1 - (1 - pct_vaccinated / 100) * pct_breakthroughs / ((1 - pct_breakthroughs / 100) * pct_vaccinated) ) . . Warning: This is not a not a sophisticated epidemiological model but simply serves to illustrated the relationship to Bayes theorem (which will also be important for more realistic models). This model should only be used to build some rough intuition about the relationship but by itself is not sufficient for decision making. Some limitations of the presented model for the presented model: . unreported cases of infections and breakthroughs | fraudulent vaccinations | multiple infection scenarios | different virus variants throughout time | ... | . Simple interpretation . One direct observation about this formula is the symmetry between $q$ and $b$ in denominator and numerator. This implies that when $q=b$ the effectiveness of the vaccination is 0! This makes intuitive sense because it means that when the chance of a infected person being vaccinated is the same as the chance of an arbitrary person being vaccinated, then vaccinations don&#39;t seem to help. . This also means that as long as the rate of breakthrough infections is below the vaccination rate, the vaccines have a real effect! . Application to German data . The German RKI published data (page 21) on breakthrough infections and percentages of vaccinated at the beginning of Nov 21. . rki_data . age 12-17 18-59 &gt;= 60 . calendar weeks (2021) 05-43 40-43 05-43 40-43 05-40 40-43 . percentage breakthrough infections 1.5 | 4.2 | 12.4 | 39.7 | 18.9 | 60.5 | . percentage of the population that is fully vaccinated 38.7 | NaN | 71.9 | NaN | 84.9 | NaN | . As the vaccination numbers were mostly stagnating later during 2021 we&#39;ll use the vaccination percentages from week 05-43 to approximate the ones from 40-43 (that might be a little higher). We then use the formula derived above to calculate the average vaccine efficiency . ( rki_data.T.fillna(method=&quot;ffill&quot;) .assign( average_vaccine_effectiveness_pct=lambda df: avg_vaccine_efficiency( df[&quot;percentage of the population that is fully vaccinated&quot;], df[&quot;percentage breakthrough infections&quot;], ) ) .applymap(lambda x: round(x, 1)) .T ) . age 12-17 18-59 &gt;= 60 . calendar weeks (2021) 05-43 40-43 05-43 40-43 05-40 40-43 . percentage breakthrough infections 1.5 | 4.2 | 12.4 | 39.7 | 18.9 | 60.5 | . percentage of the population that is fully vaccinated 38.7 | 38.7 | 71.9 | 71.9 | 84.9 | 84.9 | . average_vaccine_effectiveness_pct 97.6 | 93.1 | 94.5 | 74.3 | 95.9 | 72.8 | . One can see that when vaccinated population is high, a larger number of breakthrough infections does not contradict a larger efficiency. The reason why many people don&#39;t find this intuitive is that non-linear relationships are not very intuitive in general. As such it helps to visualize the relationship. . The next graphic shows the dependency between the percentage of breakthrough infections and the effectiveness of the vaccination for various overall percentages of vaccination. . Plots of the breakthrough/efficiency relationship . We can clearly see that the relationship become very non-linear the higher the percentage of vaccinated people. The as explained in our earlier interpretation the vaccination rate can easily be identified by looking where a particular curve crosses 0. . Summary . We illustrated how Bayes&#39; theorem can shed some light on the relationship between breakthrough infections and vaccine efficiency. . It is important to realize that this relationship becomes highly non-linear at high vaccination rates, even for effective vaccines. .",
            "url": "https://codes.correlaid.org/python/jupyter/covid/bayes/2021/11/28/bayes_vaccination.html",
            "relUrl": "/python/jupyter/covid/bayes/2021/11/28/bayes_vaccination.html",
            "date": " • Nov 28, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Convert RMarkdown files to Jupyter Notebook",
            "content": "Many of the people at CorrelAid come from a R background. Even you (the reader) could be more familiar to the R world than to the Python one. To help you all to easily display your work on the Correlaid blog, here we will introduce a couple of methods to convert R Mardown files into Jupyter Notebook ones. . These are: . Jupytext: a Python package that provides two-way conversion between Jupyter Notebooks and several other text-based formats like Markdown documents or scripts. | Notedown: a simple tool to create IPython notebooks from markdown (and r-markdown). | . Working with Jupytext . Installation . To install Jupytext, open a terminal (Unix) or a command line window (Windows) and run: . pip install jupytext --upgrade . If you use conda instead, run: . conda install jupytext -c conda-forge . Convert a R Markdown file . Once the package is installed, using the same Python distro that is used for your Jupyter Notebook/JupyerLab, you can convert a R Markdown file by following these steps: . Open a terminal (Unix) or a command line window (Windows). | Navigate to your script.Rmd file location. | Run jupytext --to notebook script.Rmd to have a script.ipynb file. | Make sure that the conversion is correct by opening the newly created notebook via Jupyter Notebook/JupyterLab. | Share your content on the CorrelAid blog! | Please refer to the project page for further references. . Working with notedown . Installation . To install notedown, open a terminal (Unix) or a command window (Windows) and run: . pip install notedown . Convert a R Markdown file . Once the package is installed, make sure that you have knitr instaled as well with your R instance. In a terminal (Unix) or command window (Windows) run: . notedown script.Rmd --knit &gt; script.ipynb . Before sharing your content, make sure that the conversion is correct by opening the newly created notebook via Jupyter Notebook/JupyterLab. . Please refer tot he project page for further references. .",
            "url": "https://codes.correlaid.org/first%20steps/r/jupyter/2021/03/02/Convert-Rmd-files-to-Jupyter-Notebook.html",
            "relUrl": "/first%20steps/r/jupyter/2021/03/02/Convert-Rmd-files-to-Jupyter-Notebook.html",
            "date": " • Mar 2, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Madrid Mobility Data",
            "content": "Madrid (CRTM: Consorcio Regional de Transportes de Madrid), unlike most other cities, provides a rich open data set on survey data on mobility behaviour of ~85.000 people with ~220.000 individual trips. . The description and results can be found here (in spanish): https://www.crtm.es/media/712934/edm18_sintesis.pdf . For our article we did some preprocessing: . - merging the different data sets - translate Spanish codes to English and join with data - simplify some answer options (e.g. modal choice or trip motive) . If you want to use this data set for your own project, this preproessing might help you get started. . import os import requests import pandas as pd import numpy as np from datetime import time %matplotlib inline . Download the data . Download all four datasets from here: https://crtm.maps.arcgis.com/apps/MinimalGallery/index.html?appid=a60bb2f0142b440eadee1a69a11693fc and store in data raw . path = os.getcwd() print(&quot;Current working directory:&quot;, path) # create sub-folders in &#39;data&#39; folder for x in [&#39;raw&#39;, &#39;interim&#39;, &#39;processed&#39;]: temp = os.path.join(&#39;../data/&#39;, x) try: os.mkdir(temp) except: print(&#39;Folder&#39;, temp, &#39;already exists.&#39;) # download and save raw datasets # HOGARES url = &#39;https://crtm.maps.arcgis.com/sharing/rest/content/items/d9e8c48ae6a1474faa34083239007307/data&#39; r = requests.get(url, allow_redirects=True) output = open(&#39;../data/raw/EDM2018HOGARES.xlsx&#39;, &#39;wb&#39;) output.write(r.content) output.close() # INDIVIDUOS url = &#39;https://crtm.maps.arcgis.com/sharing/rest/content/items/07dad41b543641d3964a68851fc9ad11/data&#39; r = requests.get(url, allow_redirects=True) output = open(&#39;../data/raw/EDM2018INDIVIDUOS.xlsx&#39;, &#39;wb&#39;) output.write(r.content) output.close() # VIAJES url = &#39;https://crtm.maps.arcgis.com/sharing/rest/content/items/6afd4db8175d4902ada0803f08ccf50e/data&#39; r = requests.get(url, allow_redirects=True) output = open(&#39;../data/raw/EDM2018VIAJES.xlsx&#39;, &#39;wb&#39;) output.write(r.content) output.close() # XETAPAS url = &#39;https://crtm.maps.arcgis.com/sharing/rest/content/items/81919e30e674422d93203a3190eafcdc/data&#39; r = requests.get(url, allow_redirects=True) output = open(&#39;../data/raw/EDM2018XETAPAS.xlsx&#39;, &#39;wb&#39;) output.write(r.content) output.close() # display content of &#39;raw&#39; folder print(&#39; n&quot;../data/raw&quot; folder contains:&#39;) print(os.listdir(&#39;../data/raw&#39;)) . Folder ../data/raw already exists. Folder ../data/interim already exists. Folder ../data/processed already exists. &#34;../data/raw&#34; folder contains: [&#39;.gitkeep&#39;, &#39;EDM2018HOGARES.xlsx&#39;, &#39;EDM2018INDIVIDUOS.xlsx&#39;, &#39;EDM2018VIAJES.xlsx&#39;, &#39;EDM2018XETAPAS.xlsx&#39;, &#39;public_transport_madrid&#39;, &#39;zt1259&#39;, &#39;zt208&#39;, &#39;zt84&#39;] . Join datasets . ind = pd.read_excel(&#39;../data/raw/EDM2018INDIVIDUOS.xlsx&#39;) hog = pd.read_excel(&#39;../data/raw/EDM2018HOGARES.xlsx&#39;) via = pd.read_excel(&#39;../data/raw/EDM2018VIAJES.xlsx&#39;, dtype = {&#39;VORIHORAINI&#39;:str, &#39;VDESHORAFIN&#39;:str}) # specify times to be read as strings etap = pd.read_excel(&#39;../data/raw/EDM2018XETAPAS.xlsx&#39;) # set unique identifiers as index for each DataFrame via.set_index([&quot;ID_HOGAR&quot;, &quot;ID_IND&quot;, &quot;ID_VIAJE&quot;], inplace = True) ind.set_index([&quot;ID_HOGAR&quot;, &quot;ID_IND&quot;], inplace = True) hog.set_index(&quot;ID_HOGAR&quot;, inplace = True) etap.set_index([&quot;ID_HOGAR&quot;, &quot;ID_IND&quot;, &quot;ID_VIAJE&quot;], inplace =True) . people = hog.join(ind, lsuffix = &quot;_hog&quot;, rsuffix = &quot;_ind&quot;) trips = people.join(via, rsuffix = &quot;_via&quot;) legs = trips.join(etap, rsuffix = &quot;_etap&quot;) # people is a dataset of households and unique people people.to_csv(r&#39;.. data interim people.csv&#39;, index = False) # trips includes households, people and unique trips trips.to_csv(r&#39;.. data interim trips.csv&#39;, index = False) # legs includes households, people, trips and unique legs legs.to_csv(r&#39;.. data interim legs.csv&#39;, index = False) ## you likely want to work with &quot;trips&quot; for analysing mobility patterns. . join codes for survey answers . codes_hog = pd.read_excel (&#39;../data/raw/EDM2018HOGARES.xlsx&#39;, sheet_name = 1) codes_ind = pd.read_excel (&#39;../data/raw/EDM2018INDIVIDUOS.xlsx&#39;, sheet_name = 1) codes_via = pd.read_excel (&#39;../data/raw/EDM2018VIAJES.xlsx&#39;, sheet_name = 1) codes_etap = pd.read_excel (&#39;../data/raw/EDM2018XETAPAS.xlsx&#39;, sheet_name = 1) # append all codes to a single file codes = codes_hog.append(codes_ind, ignore_index = True, sort = False ).append(codes_via, ignore_index = True, sort = False ).append(codes_etap, ignore_index = True, sort = False ).drop(columns = [&quot;Unnamed: 0&quot;, &quot;Unnamed: 1&quot;]) # forward fill varibale and specification to work as proper table (and not only visually in Excel) codes[&quot;VARIABLE&quot;] = codes.VARIABLE.fillna(method = &quot;ffill&quot;) codes[&quot;ESPECIFICACIÓN&quot;] = codes[&quot;ESPECIFICACIÓN&quot;].fillna(method = &quot;ffill&quot;) # split code and value codes[&#39;CODE&#39;], codes[&#39;VALUE&#39;] = codes[&#39;VALORES&#39;].str.split(&quot;.&quot;, 1).str codes[&#39;CODE&#39;], unused = codes.CODE.str.split(&quot; &#39;&quot;, 1).str codes.drop(index = codes[codes.VARIABLE.isna()].index, inplace = True) codes.to_csv(&#39;../data/interim/codes.csv&#39;, index = False) . Translate codes by using Google Sheets. . . (Google API Package for Python wasnt working? https://pypi.org/project/googletrans/ ) Option to use the Google translate API (https://cloud.google.com/translate/docs/reference/rest) but for this use case wasnt really worth to set it up. . Translation is stored in data processed codes_translated.csv | . create new variables . # trips = pd.read_csv(r&#39;.. data interim trips.csv&#39;, dtype = {&#39;VORIHORAINI&#39;:str, &#39;VDESHORAFIN&#39;:str}) . codes = pd.read_csv(r&#39;.. data processed codes_translated.csv&#39;, dtype = {&#39;CODE&#39;: float}) . Set value of sex to &#39;Man&#39; and &#39;Woman&#39; instead of numbers. . code_sex = codes[codes.VARIABLE == &quot;C2SEXO&quot;][[&#39;CODE&#39;, &#39;VALUE_en&#39;]].rename({&#39;VALUE_en&#39;: &#39;sex&#39;}, axis = 1) trips = trips.join(code_sex.set_index(&#39;CODE&#39;), on = &quot;C2SEXO&quot;) . Compute duration from start (VORIHORAINI) and endtime (VDESHORAFIN). . mins = pd.to_numeric(trips.VDESHORAFIN.str.slice(2)) - pd.to_numeric(trips.VORIHORAINI.str.slice(2)) hours = (pd.to_numeric(trips.VDESHORAFIN.str.slice(0, 2)) - pd.to_numeric(trips.VORIHORAINI.str.slice(0, 2))) * 60 trips[&quot;duration&quot;] = mins + hours . Create a datetime object from string. . trips[&#39;start_time&#39;] = pd.to_datetime(trips.VORIHORAINI, format = &#39;%H%M&#39;).dt.time . Endtimes sometimes exceeds 24 hours. (If a trip starts at 11pm and ends at 1am, then the endtime is 2500). Datetime does not work with times &gt; 24h. Therefore this needs to be fixed. . trips[&quot;end_time&quot;] = np.where(pd.to_numeric(trips[&#39;VDESHORAFIN&#39;]) &gt; 2400, pd.to_numeric(trips[&#39;VDESHORAFIN&#39;]) - 2400, pd.to_numeric(trips[&#39;VDESHORAFIN&#39;])) . trips[&#39;end_time&#39;] = pd.to_datetime(trips.end_time, format = &#39;%H%M&#39;, errors = &#39;coerce&#39;).dt.time . trips[&quot;speed&quot;] = np.where(trips.duration != 0, # skip if start and Endtime is the same trips.DISTANCIA_VIAJE / (trips.duration / 60), np.NaN) . trips.loc[trips.speed &gt; 150, &quot;speed&quot;] = None . The original survey has a 24 different modes: . 1: Renfe Cercanías (train) | 2: Autobus interurbano (intercity bus) | 3: Autobus urbano otro municipio (urban bus other municipalities) | 4: Metro (metro) | 5: Metro ligero/tranvía (light train) | 6: Autobus urbano Madrid EMT (urban bus Madrid) | 7: Resto renfe (train) | 8: Autobus discrecional (unscheduled bus) | 9: Autobus de largo recorrido (intercity bus) | 10: Taxi (taxi) | 11: Coche conductor particular (private car) | 12: Coche conductor empresa (business car) | 13: Coche conductor alquiler sin conductor (car rental without driver) | 14: Coche pasajero particular (private car passenger) | 15: Coche pasajero empresa (business car passenger) | 16: Coche pasajero alquiler con conductor (car rental with driver) | 17: Moto/ciclomotor particular (private motorcycle) | 18: Moto/ciclomotor publica (public motorcycle) | 19: Moto/ciclomotor empresa (business motorcycle) | 20: Bicicleta particular (private bicycle) | 21: Bicicleta publica (public bicycle) | 22: Bicicleta empresa (business bicycle) | 23: Otros (other) | 24: Andando/pie (walk) | . These are simplified to four modes: . public transport | car | other (including motorbicycle and bicycle) | walk | . code_mode = codes[codes[&#39;VARIABLE&#39;] == &#39;MODO_PRIORITARIO&#39;][[&#39;CODE&#39;, &#39;VALUE_en&#39;]].drop_duplicates() trips = trips.join(code_mode.set_index(&#39;CODE&#39;), on = &#39;MODO_PRIORITARIO&#39;, how = &quot;left&quot;).reset_index() trips.rename({&quot;VALUE_en&quot;: &quot;mode&quot;}, axis = &quot;columns&quot;, inplace = True) trips[&quot;mode_simple&quot;] = trips.MODO_PRIORITARIO trips.loc[trips.MODO_PRIORITARIO &lt; 10, &quot;mode_simple&quot;] = &quot;public transport&quot; trips.loc[(trips.MODO_PRIORITARIO &gt; 9) &amp; (trips.MODO_PRIORITARIO &lt; 17), &quot;mode_simple&quot;] = &quot;car&quot; trips.loc[(trips.MODO_PRIORITARIO &gt; 16) &amp; (trips.MODO_PRIORITARIO &lt; 24), &quot;mode_simple&quot;] = &quot;other&quot; trips.loc[trips.MODO_PRIORITARIO == 24, &quot;mode_simple&quot;] = &quot;walk&quot; . The original survey has 12 trip motives: . 1: Casa (Home) | 2: Trabajo (Work) | 3: Gestión de trabajo (work management?) | 4: Estudio (study) | 5: Compras (Purchases) | 6: Médico (Doctor) | 7: Acompañamiento a otra persona (Acompany another person) | 8: Ocio (leisure) | 9: Deporte/ dar un paseo (sport) | 10: Asunto personal (personal matter) | 11: Otro domicilio (other residence) | 12: Otros (other) | . These are simplified to 5 trip motives: . work | study | car / errand | leisure | other | . code_motive = codes[codes[&#39;VARIABLE&#39;] == &#39;MOTIVO_PRIORITARIO&#39;][[&#39;CODE&#39;, &#39;VALUE_en&#39;]] trips = trips.join(code_motive.set_index(&#39;CODE&#39;), on = &#39;MOTIVO_PRIORITARIO&#39;).reset_index() trips.rename({&quot;VALUE_en&quot;: &quot;motive&quot;}, axis = &quot;columns&quot;, inplace = True) trips.loc[:,&quot;motive_simple&quot;] = trips.motive trips.loc[(trips.MOTIVO_PRIORITARIO == 2) | (trips.MOTIVO_PRIORITARIO == 3), &quot;motive_simple&quot;] = &quot;work&quot; trips.loc[((trips.MOTIVO_PRIORITARIO &gt;= 5) &amp; (trips.MOTIVO_PRIORITARIO &lt;= 8) | (trips.MOTIVO_PRIORITARIO == 10)), &quot;motive_simple&quot;] = &quot;care / errand&quot; trips.loc[(trips.MOTIVO_PRIORITARIO &gt;= 8) &amp; (trips.MOTIVO_PRIORITARIO &lt;= 9), &quot;motive_simple&quot;] = &quot;leisure&quot; trips.loc[(trips.MOTIVO_PRIORITARIO == 1) | (trips.MOTIVO_PRIORITARIO == 11) | (trips.MOTIVO_PRIORITARIO == 12), &quot;motive_simple&quot;] = &quot;other&quot; . Traffic can be divided into three main categories time wise: . rush hour (usually morning and afternoon) | off-peak (usually during the day) | low traffic (usually night and weekend) | . A new category &#39;daytime&#39; is created for this. . conditions = [ ((trips.start_time &gt;= time(7)) &amp; (trips.start_time &lt; time(9))) | ((trips.start_time &gt;= time(17)) &amp; (trips.start_time &lt; time(20))), (trips.start_time &gt;= time(9)) &amp; (trips.start_time &lt; time(17)), (trips.start_time &gt;= time(20)) | (trips.start_time &lt; time(7)) ] choices = [&#39;rush hour (HVZ)&#39;, &#39;off-peak hour (NVZ)&#39;, &#39;low traffic time (SVZ)&#39;] trips[&#39;daytime&#39;] = np.select(conditions, choices, default= None) . Round age to 5 to get larger groups with the same age and make it easier to work with. . trips[&quot;rounded_age&quot;] = 5 * (trips.EDAD_FIN / 5).round() . Create four different age groups: . kids (&lt;= 18) | young adults (18-25) | adults (25 - 55) | seniors (&gt; 55) | . conditions = [ trips.EDAD_FIN &lt;= 18, (trips.EDAD_FIN &gt; 18) &amp; (trips.EDAD_FIN &lt;= 25), (trips.EDAD_FIN &gt; 25) &amp; (trips.EDAD_FIN &lt;= 55), trips.EDAD_FIN &gt; 55] choices = [&#39;kids&#39;, &#39;young adults&#39;, &#39;adults&#39;, &#39;seniors&#39;] trips[&#39;age_group&#39;] = np.select(conditions, choices, default= None) . trips.drop([&quot;index&quot;], axis = 1, inplace = True) . Store preprocessed data to csv-File to work with in your use case. . trips.to_csv(r&#39;.. data processed trips_custom_variables.csv&#39;, index = False) .",
            "url": "https://codes.correlaid.org/mobility/preprocessing/2021/02/28/Preprocessing_Madrid_Mobility_Data.html",
            "relUrl": "/mobility/preprocessing/2021/02/28/Preprocessing_Madrid_Mobility_Data.html",
            "date": " • Feb 28, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Setup Jupyter for blogging",
            "content": "Background info on Jupyter&#39;s intentions . Jupyer and its usefulness with R have been covered in a previous blogpost here. Go back and have a quick look at what you are capable to do in Jupyter. . This post will help you setup this framework on your local station and start creating your posts. For further details, please refer to the main page of the projects. . In this post we are assuming that you, the reader, is starting from zero and have no Python installation (or literacy) on your compuater. . There are many ways to set you toolkit up for hardcore blogging at CorrelAid. In here we will cover: . JupyterLab vs Jupyter Notebook. | How to setup Python. | How to setup JupyterLab or Jupyter Notebook. | . Let&#39;s get started! . JupyterLab vs Jupyter Notebook . If you are completely new to the Python world, you might have not heard of JupyterLab or Jupyter Notebook. That&#39;s alright, the bottom line is that both can be used to create your IPython notebooks (and therefore posts) for our blog! . Both are web-based interface, although JupyterLab feels more like a complete IDE (like RStudio) while Jupyter Notebook are documents that combine live runnable code with narrative text (Markdown), equations (LaTeX), images, interactive visualizations and other rich output. Pick you poison and follow us to the next step. . How to setup Python . Installing Python can be done in multiple ways depending on your operative system, but also on the choice of package management system that you prefer to use. In here we present pip and conda; they have different ways to be installed. . What is &quot;pip&quot;? . PIP is an acronym that stands for &quot;PIP Installs Packages&quot; or &quot;Preferred Installer Program&quot;. It&#39;s a command-line utility that allows you to install, reinstall, or uninstall PyPI packages with a simple and straightforward command: pip. . How to get &quot;pip&quot; on my computer? . We reccomend to use Python 3.x versions, in this way pip will already be available. Here are the installation for Windows, Linux/UNIX and MacOS X. . What is &quot;conda&quot;? . Anaconda is a package manager, an environment manager, a Python/R data science distribution, and a collection of over 7,500+ open-source packages. Anaconda is free and easy to install, and it offers free community support. . How to get &quot;conda&quot; on my computer? . This system is very broad and organic, please refer to the installation page of the project to choose between the full suite or the reduced one (Miniconda) . How to setup JupyterLab or Jupyter Notebook . After choosing the Python package manager and between the two methods to create your IPython notebooks, let&#39;s have a quick look on how to run the install. . Install Jupyter Notebook . Install Jupyter Notebook via pip, you can run in console: . pip install notebook . Or via conda: . conda install -c conda-forge notebook . Run the following command at the Terminal (Mac/Linux) or Command Prompt (Windows) to open your notebooks: . jupyter notebook . Install JupyterLab . Install JupyterLab via pip, you can run in console: . pip install jupyterlab . Or via conda: . conda install -c conda-forge jupyterlab . Run the following command at the Terminal (Mac/Linux) or Command Prompt (Windows) to open the web interface: . jupyter-lab . Please refer to the project documentation for more information. Happy blogging! .",
            "url": "https://codes.correlaid.org/first%20steps/r/jupyter/2021/02/25/Setup_Jupyter_for_blogging.html",
            "relUrl": "/first%20steps/r/jupyter/2021/02/25/Setup_Jupyter_for_blogging.html",
            "date": " • Feb 25, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "R in jupyter notebooks",
            "content": "Jupyter&#39;s intentions . Jupyter was orignally designed as an interactive environment for Julia, Python and R. This is even reflected in the name JuliapythonR, even if they snug an extra e in there. In the python data science community Jupyter is widely used, while the R community uses Rstudio as the standard IDE and it is what most newcomers to R are introduced to initially. . Here we&#39;ll give an example of how to use Jupyter as an alternative to Rstudio. This should not be misunderstood. Rstudio is a great tool and we don&#39;t want to downplay its importance. The main advantage of using Jupyter is access to a set of tools that are specifically build for Jupyter notebooks. Many of these tools were developed with python in mind, but the multi language usability of Jupyter allows us to make use of them in R. This post is an example in the sense, that it was written for fastPages, a notebook blogging framework. . IRkernel and code execution . In order to run R code in Jupyter an R kernel needs to be registered with Jupyter. This requires an existing R installation and the additional R package IRkernel. This module can then register the R installation as a kernel for Jupyter. Subsequently the R kernel can be selected when running a Jupyter notebook which allows the execution from R code in code cells. . The following is a simple example of loading some standard R packages, some data and creating a plot. . require(&#39;tidyverse&#39;); data(mtcars) . mtcars %&gt;% arrange(&#39;cyl&#39;) %&gt;% ggplot(aes(x=mpg,y=cyl)) + geom_point() . Interactive features . Nowadays there are many libraries that create interactive plots and maps that are driven by JavaScript. Many of these naturally work well together with Jupyter notebooks ans fastPages. An example for this is leaflet. A popular JavaScript library for displaying locations on maps. R has a package with the same name also supporting this library. The following is a standard example from the package website. . library(leaflet) m &lt;- leaflet() %&gt;% addTiles() %&gt;% # Add default OpenStreetMap map tiles addMarkers(lng=174.768, lat=-36.852, popup=&quot;The birthplace of R&quot;) m # Print the map . &lt;!doctype html&gt; . . Google Colab and Reproducibility . Another nice feature of fastPages is that it directly support links to Google Colab and Binder. These tools allow readers of Jupyter Notebooks/fastPages blog posts to execute the code themselves without needing a local Jupyter setup local installation. Unfortunately Binder does not support R kernels at this point, but Google Colab does. There posts (like this one) can be opened there and can be executed and modified. One could even use Google Colab to create blog posts to begin with. The following blog post gives a few more details on that. There are a few limitation to Google Colab however, for instance some interactive libraries like leaflet don&#39;t display correctly. . Conclusion . We showed how to using Jupyter notebooks for R programming is possible and opens up many possibilities to the R user. Out of the features that this brings to the R user, we want to highlight most, that it allows to create blog posts using fastPages, just like this one. .",
            "url": "https://codes.correlaid.org/first%20steps/r/jupyter/2021/02/18/R_in_jupyter.html",
            "relUrl": "/first%20steps/r/jupyter/2021/02/18/R_in_jupyter.html",
            "date": " • Feb 18, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "First steps in data exploration",
            "content": "import requests import os data_url = &#39;http://www.berlin.de/sen/finanzen/dokumentendownload/haushalt/haushaltsplan-/haushaltsplan-2018-2019/20180125_dhh18_19.csv&#39; file = &#39;berlin_budget.csv&#39; if not os.path.exists(file): with open(file, &#39;wb&#39;) as file: resp = requests.get(data_url) file.write(resp.content) . For sharing more complex data analyses on fastPages, a step like this might not be very interesting to the ready and might quite likely be hidden by starting the cell with #hide. . Small nuisance when loading data . Now even with a .csv file locally present the are a few annoying errors that that might prevent somebody from simply importing. What we would all like to to do, is to simply call pd.read_csv(&#39;berlin_budget.csv&#39;) . Encoding . This is probably one of the most annoying data aspects that data scientists sometimes have to deal with. Unfortunately when encoding errors occur one has to deal with them, and manually inspecting the data files in a text editor or excel, generally does not help loading them into python. In practical terms most files today are encoded in what is called UTF-8, and that is the standard that python applications assume. The good thing is most functions that deal with files, like open or pd.read_csv let you specify the encoding. Unfortunately it can be overwhelming what to pick when confronted with this issue for the first time. Legacy windows applications, which have been and probably still are present in many public institutions and agencies, used different encodings. Most commonly (for western files) an encoding called CP1252 was used. Therefore we will also specify this encoding when reading in the data. . Tip: When you get encoding errors for files using Latin characters, try CP1252. . Separator . The next issue with this file is the separator, which is actually not ,. In the best cases one can still read in the files and then realize this later on. Sometimes this can fail, however. As is the case with our budget data set. In such a case the file needs to be either inspected to find the right separator, or common separators can simply be tried (for instance [&#39;,&#39;,&#39;;&#39;,&#39; t&#39;]). In this case ; lets us load the data, which is a typical separator for .csv files in Germany. . Number formatting . While being annoying as well number formatting usually does not prevent one from loading data, but simply making manipulating it more difficult. Here we again set the typical German values decimal = &#39;,&#39; and thousands = &#39;.&#39;. . import pandas as pd berlin_budget_raw = pd.read_csv(&#39;berlin_budget.csv&#39;, sep=&#39;;&#39;, decimal=&#39;,&#39;, thousands=&#39;.&#39;, encoding=&#39;CP1252&#39;) . First steps with raw data . Now that we have imported the raw data we&#39;ll go through some typical starting points. These are . checking the shape | looking at example rows | looking at the data types | checking for missing data | check different values for discrete variables | . Shape . A first helpful thing is usually, to look at the shape of the data with berlin_budget_raw.shape. For this data set it shows that we have 20828 rows and 25 columns. This means that there are more rows, than we can look at manually and there are more columns than we can conveniently look it. This is often the case and the data was chosen as an example of how to deal with this. The strategy to deal with the rows, is to use function that help us analyze the entire content to get an impression. The strategy for the columns is to use additional information we find to exclude as many columns as possible for initial analyses, to make the data easier to handle. . Example rows . There are several methods to get sample rows of a dataframe. The most typical ones in pandas are .head, .tail and .sample, which return the first 5 rows, last 5 rows or a random 5 rows on a call. . berlin_budget_raw.head() . Typ Bezeichnung Bereich Bereichsbezeichnung Einzelplan Einzelplanbezeichnung Kapitel Kapitelbezeichnung Hauptgruppe Hauptgruppenbezeichnung ... Hauptfunktionsbezeichnung Oberfunktion Oberfunktionsbezeichnung Funktion Funktionsbezeichnung Titelart Titel Titelbezeichnung Ansatz 2018 in € Ansatz 2019 in € . 0 2.0 | Verfassungsorgane | 30.0 | Hauptverwaltung | 1.0 | Abgeordnetenhaus | 100.0 | Abgeordnetenhaus | 1.0 | Verwaltungseinnahmen, Einnahmen aus Schuldendi... | ... | Allgemeine Dienste | 1.0 | Politische Führung und zentrale Verwaltung | 11.0 | Politische Führung | Einnahmetitel | 11201.0 | Geldstrafen, Geldbußen, Verwarnungs- und Zwang... | 1000.0 | 1000.0 | . 1 2.0 | Verfassungsorgane | 30.0 | Hauptverwaltung | 1.0 | Abgeordnetenhaus | 100.0 | Abgeordnetenhaus | 1.0 | Verwaltungseinnahmen, Einnahmen aus Schuldendi... | ... | Allgemeine Dienste | 1.0 | Politische Führung und zentrale Verwaltung | 11.0 | Politische Führung | Einnahmetitel | 11906.0 | Ersatz von Fernmeldegebühren | 1000.0 | 1000.0 | . 2 2.0 | Verfassungsorgane | 30.0 | Hauptverwaltung | 1.0 | Abgeordnetenhaus | 100.0 | Abgeordnetenhaus | 1.0 | Verwaltungseinnahmen, Einnahmen aus Schuldendi... | ... | Allgemeine Dienste | 1.0 | Politische Führung und zentrale Verwaltung | 11.0 | Politische Führung | Einnahmetitel | 11961.0 | Erstattung von Steuerbeträgen | 1000.0 | 1000.0 | . 3 2.0 | Verfassungsorgane | 30.0 | Hauptverwaltung | 1.0 | Abgeordnetenhaus | 100.0 | Abgeordnetenhaus | 1.0 | Verwaltungseinnahmen, Einnahmen aus Schuldendi... | ... | Allgemeine Dienste | 1.0 | Politische Führung und zentrale Verwaltung | 11.0 | Politische Führung | Einnahmetitel | 11979.0 | Verschiedene Einnahmen | 10000.0 | 10000.0 | . 4 2.0 | Verfassungsorgane | 30.0 | Hauptverwaltung | 1.0 | Abgeordnetenhaus | 100.0 | Abgeordnetenhaus | 1.0 | Verwaltungseinnahmen, Einnahmen aus Schuldendi... | ... | Allgemeine Dienste | 1.0 | Politische Führung und zentrale Verwaltung | 11.0 | Politische Führung | Einnahmetitel | 12401.0 | Mieten für Grundstücke, Gebäude und Räume | 45000.0 | 45000.0 | . 5 rows × 25 columns . The first 5 rows show us many numeric and string columns and show that repeating string columns seem to coincide with repeating numeric columns. . Data types . It often helps to look at the data types of the data frame using berlin_budget_raw.dtypes. We find . berlin_budget_raw.dtypes . Typ float64 Bezeichnung object Bereich float64 Bereichsbezeichnung object Einzelplan float64 Einzelplanbezeichnung object Kapitel float64 Kapitelbezeichnung object Hauptgruppe float64 Hauptgruppenbezeichnung object Obergruppe float64 Obergruppenbezeichnung object Gruppe float64 Gruppenbezeichnung object Hauptfunktion float64 Hauptfunktionsbezeichnung object Oberfunktion float64 Oberfunktionsbezeichnung object Funktion float64 Funktionsbezeichnung object Titelart object Titel float64 Titelbezeichnung object Ansatz 2018 in € float64 Ansatz 2019 in € float64 dtype: object . . This shows again that many columns with similar names come as a float column together with another object (string) column. We can also see that there are two columns containing €, which are likely to contain the actual budget data. This already suggest the following strategy for reducing the columns. For each pair of columns that seem to belong together, we might be able to drop one of the two for initial analyses. For the budget column, it is probably a good idea to start with only one of them. . Warning: Data types might be wrong if there were any problems or ambiguities when reading in a file. When in doubt columns with an object (string) data type should be inspected manually to check whether they really contain text. . Missing values . Missing data can cause all sorts of issues. And it is a good idea to check for missing data and deal with it. One of the easiest checks is the following, which is a trick based on the ability to sum boolean values. . berlin_budget_raw.isna().sum() . Typ 1 Bezeichnung 1 Bereich 1 Bereichsbezeichnung 1 Einzelplan 1 Einzelplanbezeichnung 1 Kapitel 1 Kapitelbezeichnung 1 Hauptgruppe 1 Hauptgruppenbezeichnung 1 Obergruppe 1 Obergruppenbezeichnung 1 Gruppe 1 Gruppenbezeichnung 1 Hauptfunktion 1 Hauptfunktionsbezeichnung 1 Oberfunktion 1 Oberfunktionsbezeichnung 1 Funktion 1 Funktionsbezeichnung 1 Titelart 1 Titel 1 Titelbezeichnung 1 Ansatz 2018 in € 1 Ansatz 2019 in € 1 dtype: int64 . . This shows that there is exactly one missing value for each row, which is not too bad. Additionally it suggests, that there is probably an empty row, instead of randomly missing data. And true enough, if we were to check berlin_budget_raw.tail() we would see that every value in the last row is missing. Such cases are easy to deal with, because we can just drop that row. . Different values . One of the most useful approaches to get an impression of categorical data is to count how many different values are in a columns. The build in .nunique() method in pandas does exactly this. To get a little more structure into it, it helps to sort the results right away. . berlin_budget_raw.nunique().sort_values() . Titelart 2 Typ 3 Bezeichnung 3 Hauptfunktionsbezeichnung 9 Hauptfunktion 9 Hauptgruppenbezeichnung 10 Hauptgruppe 10 Bereichsbezeichnung 13 Bereich 13 Einzelplan 31 Einzelplanbezeichnung 31 Obergruppenbezeichnung 49 Oberfunktionsbezeichnung 50 Oberfunktion 52 Obergruppe 58 Funktionsbezeichnung 138 Funktion 138 Gruppenbezeichnung 150 Gruppe 173 Kapitelbezeichnung 284 Kapitel 285 Titel 1739 Titelbezeichnung 1925 Ansatz 2018 in € 3444 Ansatz 2019 in € 3449 dtype: int64 . . There are many interesting information here. First of all Titelart only has two different values. For budget data this means that it is a good candidate to indicate whether something is an expense or a funding source. Furthermore many of the categories with similar names have very similar distinct value counts, supporting our earlier strategy idea of only using one of the in first analyses. We also see that some categories have only a small number of different values. These are more suited for initial analyses, because they keep it simple and allow an easy inspection of aggregated data, as will be illustrated below. . . Tip: When calculating several column based quantites pd.concat can help to summarize them. . #collapse-output pd.concat( [ berlin_budget_raw.dtypes.rename(&#39;dtypes&#39;), berlin_budget_raw.isna().sum().rename(&#39;missing&#39;), berlin_budget_raw.nunique().rename(&#39;distinct_values&#39;) ] ,axis=1 ).sort_values(&#39;distinct_values&#39;) . . dtypes missing distinct_values . Titelart object | 1 | 2 | . Typ float64 | 1 | 3 | . Bezeichnung object | 1 | 3 | . Hauptfunktionsbezeichnung object | 1 | 9 | . Hauptfunktion float64 | 1 | 9 | . Hauptgruppenbezeichnung object | 1 | 10 | . Hauptgruppe float64 | 1 | 10 | . Bereichsbezeichnung object | 1 | 13 | . Bereich float64 | 1 | 13 | . Einzelplan float64 | 1 | 31 | . Einzelplanbezeichnung object | 1 | 31 | . Obergruppenbezeichnung object | 1 | 49 | . Oberfunktionsbezeichnung object | 1 | 50 | . Oberfunktion float64 | 1 | 52 | . Obergruppe float64 | 1 | 58 | . Funktionsbezeichnung object | 1 | 138 | . Funktion float64 | 1 | 138 | . Gruppenbezeichnung object | 1 | 150 | . Gruppe float64 | 1 | 173 | . Kapitelbezeichnung object | 1 | 284 | . Kapitel float64 | 1 | 285 | . Titel float64 | 1 | 1739 | . Titelbezeichnung object | 1 | 1925 | . Ansatz 2018 in € float64 | 1 | 3444 | . Ansatz 2019 in € float64 | 1 | 3449 | . Simplified Data . With the analyses above we create the following simplified data as follows. . We only keep categorical columns with a small number of values (&lt;=13) | We only keep the string version of the categorical category | We only keep the 2018 budget column | We drop the missing data | We store results in berlin_budget and have a look at some samples. . berlin_budget.sample(5) . Titelart Bezeichnung Hauptfunktionsbezeichnung Hauptgruppenbezeichnung Bereichsbezeichnung Ansatz 2018 in € . 8616 Einnahmetitel | Bezirke | Soziale Sicherung, Familie und Jugend, Arbeits... | Einnahmen aus Zuweisungen und Zuschüssen mit A... | Friedrichshain-Kreuzberg | 1000.0 | . 9789 Einnahmetitel | Bezirke | Soziale Sicherung, Familie und Jugend, Arbeits... | Einnahmen aus Zuweisungen und Zuschüssen mit A... | Pankow | 1000.0 | . 19183 Ausgabetitel | Bezirke | Soziale Sicherung, Familie und Jugend, Arbeits... | Ausgaben für Zuweisungen und Zuschüsse mit Aus... | Lichtenberg | 1000.0 | . 1344 Ausgabetitel | Senatsverwaltungen | Soziale Sicherung, Familie und Jugend, Arbeits... | Ausgaben für Zuweisungen und Zuschüsse mit Aus... | Hauptverwaltung | 490000.0 | . 8132 Ausgabetitel | Bezirke | Bildungswesen, Wissenschaft, Forschung, kultur... | Personalausgaben | Friedrichshain-Kreuzberg | 182000.0 | . This already simplified the data a lot and brought it down from 25 to 6 columns!. At this point one can more easily get started with more concrete analyses. In This post we restrict ourselves to two simple examples. A consistency check regarding the budget&#39;s spending and funding. And a simple overview plot about different spending. . Budget Consistency . We&#39;ve already pointed out that Titleart is a good distinguish expenses and funding sources. Such a hypotheses can easily be checked with the data. . berlin_budget.groupby(&#39;Titelart&#39;)[&#39;Ansatz 2018 in €&#39;].sum() . Titelart Ausgabetitel 2.860320e+10 Einnahmetitel 2.860320e+10 Name: Ansatz 2018 in €, dtype: float64 . This works out. With a little bit of domain knowledge about Berlin itself we realized that the column Bereichsbezeichung contains mostly the different districts of Berlin. So a reasonable followup check is to do the same analysis on a district level. This also works out. . berlin_budget.groupby([&#39;Bereichsbezeichnung&#39;,&#39;Titelart&#39;])[&#39;Ansatz 2018 in €&#39;].sum().unstack() . . Titelart Ausgabetitel Einnahmetitel . Bereichsbezeichnung . Charlottenburg-Wilmersdorf 7.046985e+08 | 7.046985e+08 | . Friedrichshain-Kreuzberg 7.189672e+08 | 7.189672e+08 | . Hauptverwaltung 1.947634e+10 | 1.947634e+10 | . Lichtenberg 8.912989e+08 | 8.912989e+08 | . Marzahn-Hellersdorf 6.990315e+08 | 6.990315e+08 | . Mitte 1.026627e+09 | 1.026627e+09 | . Neukölln 9.115210e+08 | 9.115210e+08 | . Pankow 9.365851e+08 | 9.365851e+08 | . Reinickendorf 6.343956e+08 | 6.343956e+08 | . Spandau 6.432431e+08 | 6.432431e+08 | . Steglitz-Zehlendorf 5.855771e+08 | 5.855771e+08 | . Tempelhof-Schöneberg 7.978415e+08 | 7.978415e+08 | . Treptow-Köpenick 5.770762e+08 | 5.770762e+08 | . First bar plots . A quantities dependence on a few categories can be nicely visualized using bar plots. This is another advantage of starting with columns containing few different elements when analyzing a new data set. As an illustration we pick Hauptfunktionsbezeichnung and plot spending against it. often two simple tricks help with the visualization in these cases. . The use of horizontal bar charts, because it makes category names easier to read. | Sorting the values before plotting them, which makes everything easier to compare. | In our example we get1: . ( berlin_budget .query(&#39;Titelart == &quot;Ausgabetitel&quot;&#39;) .groupby(&#39;Hauptfunktionsbezeichnung&#39;)[&#39;Ansatz 2018 in €&#39;].sum() .sort_values() .plot.barh() ) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd9d00d7890&gt; . Conclusion . This article gave a concrete example how to start tackling a real life data set. We primarily showed how to analyze and most importantly reduce the available data to get a manageable starting point. This often boils down to identifying a meaningful subset of columns that are suited for initial analysis and plots. This is particularly helpful in the absence of documentation or good domain knowledge and we hope it will hep the readers to get started with data that interests them. . 1. We also used a matplotlib style for prettier output inside the post.↩ .",
            "url": "https://codes.correlaid.org/first%20steps/python/jupyter/2021/02/11/berlin_budget.html",
            "relUrl": "/first%20steps/python/jupyter/2021/02/11/berlin_budget.html",
            "date": " • Feb 11, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://codes.correlaid.org/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://codes.correlaid.org/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "CorrelAid is a non-partisan non-profit network of data science enthusiasts who want to change the world through data science. We dedicate our work to the social sector and those organizations that strive for making the world a better place. In order to improve data literacy in society, we share our knowledge within our network and beyond and are always looking for ways to broaden our horizons. . On this blog, we collect code contributions from members of our network of over 1600 data scientists. Small code snippets, longer analyses, posts about internal projects as well as an overview over our open source contributions have a place here. . Learn more about CorrelAid here. .",
          "url": "https://codes.correlaid.org/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Open Source",
          "content": "At CorrelAid, we are big fans of Open Source. We contribute to the community through the maintenance and development of several R and Python packages that all have the aim to make data more easily accessible. In addition, we try to open source our Data4Good projects whenever GDPR allows for it. Finally, we have made our internal documentation and several of our internal tools openly accessible. . Package Development . datenguidepy: Python wrapper for the datenguide API, available on PyPi | datenguideR: R wrapper for the datenguide API | newsanchor: R wrapper for the NewsAPI, available on CRAN | pocketapi: R wrapper for the Pocket API, available on CRAN | . Data4Good projects and events . Erlassjahr: code and data for the interactive map our project team built for Erlassjahr e.V. | GLEIF relationship visualization tool: client and server, developed as part of a hackathon together with GLEIF. Learn more about the tool here | . Education . TidyTuesday: CorrelAid tidytuesday contributions | Gender equality and mobility: Code and data for the blog post about gender differences in mobility patterns | CorrelAidX Challenge: Over the course of 8 weeks in summer 2020, we called on our local chapters to use regional data, provided by the state statistical offices, from their region and submit creative data projects using the python package developed by Datenguide in collaboration with CorrelAid. You can read more about the challenge here. Commuters in Germany: Code for the interactive dashboard, developed by CorrelAidX Bremen | Moving through Berlin by bike: Code for the interactive dashboard, developed by CorrelAidX Berlin | The Datenguide Chatbot: code for a chatbot to interact with the datenguide API, developed by CorrelAidX Munich | Child wellbeing in Germany: code for an interactive jupyter notebook, developed by CorrelAidX Hamburg | Datenguide Viz: code for an interactive notebook to interact with the datenguide API for Hesse | . | CorrelCollection: Collection of materials from our annual conference, the open online data meetup series and other events and workshops | . Open Source CorrelAid Infrastructure . Hugo Website: code for our website correlaid.org | CorrelAid Code[s] Blog: code for this blog codes.correlaid.org | CorrelAid docs: internal documentation / wiki, available at docs.correlaid.org | CorrelAid Slackbot: Code for the CorrelBot who does different things in our Slack workspace. Implemented in Node.js and AWS Lambda. | projectutils: R package for coordination of our projects | projectsdb: “database” of some of our finished projects | projects-django: project management application (currently in development) | correltools: R package for various CorrelAid things (in development) | correlaidmatplotlib: CorrelAid Matplotlib style, available on PyPi | .",
          "url": "https://codes.correlaid.org/opensource/",
          "relUrl": "/opensource/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://codes.correlaid.org/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}